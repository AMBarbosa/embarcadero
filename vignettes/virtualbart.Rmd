---
title: "Using BART with a virtual species!"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{virtualbart} 
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
suppressMessages(library(tidyverse))
options(digits=2)

```


# Getting Started

So you're interested in using `embarcadero` to do species distribution modeling with Bayesian additive regression trees! That's great. BARTs are a powerful way to do machine learning and, while not a new method per se, they are very new for SDMs. 

In that BARTs are both a fairly new and niche method (so far!), I've assumed that the average reader of this vignette/average user of this package is likely to be familiar with the core concepts of species distribution modeling, including what probability of presence vs. a distribution is, what a niche is, what a pseudoabsence is, how raster data works, and things like that. If you don't, I strongly recommend some of these papers as some good but deep reading: 

- Merow, C., et al. (2014) What do we gain from simplicity versus complexity in species distribution models? Ecography, 37(12), 1267-1281.
- Elith, J., & Graham, C. H. (2009) Do they? How do they? WHY do they differ? On finding reasons for differing performances of species distribution models. Ecography 32(1), 66-77.
- Elith, J., & Leathwick, J. R. (2009). Species distribution models: ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics 40, 677-697.

Assuming you're good on SDMs, let's jump in on Bayesian Additive Regression Trees as a method! Most of the core functionality of `embarcadero` is actually a wrapper for `dbarts`, which runs the actual BART fitting process. This vignete will show you

1. How to run BARTs, and what's inside
1. How to address variable importance and selection
1. How to visualize model behavior with partial dependence plots

There's also just going to be some general comments on the process of using BARTs, the challenges to working with them, and some things that are hopefully coming next.

If you want to install, do it using devtools for now. (CRAN is going to be sorted sometime starting in early 2020.) You'll also need a few other packages in this vignette which aren't dependencies but will come up a bit:

```{r setup, echo=TRUE, message=FALSE}
#devtools::install_github('cjcarlson/embarcadero')
library(embarcadero, quietly = T)
library(dismo, quietly=T)
library(NLMR, quietly = T)
library(virtualspecies, quietly = T)
set.seed(42)
```

Doors are closing; please stand clear of the doors.

# Creating the virtual species

First, let's create an imaginary landscape. We do this using the NLMR package:

```{r}
onelandscape <- function(x) {NLMR::nlm_gaussianfield(nrow = 150,
                                                     ncol = 150,
                                                     rescale = FALSE)}
climate <- stack(lapply(c(1:8), onelandscape))
xnames <- c('x1','x2','x3','x4','x5','x6','x7','x8')
names(climate) <- xnames

plot(climate[[1]],main='An imaginary variable', 
     box=F, axes=F)
```

Next, let's make a species using the 'virtualspecies' package. Our imaginary species will only responds to variables 1-4, making variables 5-8 uninformative predictors (hopefully our model will drop them):

```{r}

# Generate the species' climatic niche

random.sp <- generateRandomSp(climate[[1:4]], 
                              # ^ These are the informative predictors
                              approach="pca",
                              relations='gaussian',
                              species.prevalence=0.5,
                              realistic.sp = TRUE,
                              PA.method='threshold')

# Generate some presences, and some absences, with imperfect detection

sp.points <- sampleOccurrences(random.sp,
                               n=250,
                               type = 'presence-absence',
                               detection.probability = 0.9)

# Extract the associated climate values

occ <- SpatialPoints(sp.points$sample.points[,c('x','y')])
occ.df <- cbind(sp.points$sample.points,
                raster::extract(climate, occ))

# Finally, let's drop the long-lats and the "Real" ground truthed presence-absence values, and just leave behind an "Observed" and the climate data

occ.df <- occ.df[,-c(1:3)]

```

Alright. Now that we have the dataset, let's get to modeling!

# Building a basic BART model

We could easily throw all our data in one model, run it on defaults, make a map, and never think about it again. There's no laws against it.

```{r}
# Check out the data structure
head(occ.df)

# Train the model
sdm <- bart(y.train=occ.df[,'Observed'],
            x.train=occ.df[,xnames],
            keeptrees = TRUE) # It's very important this is set to TRUE

# Model diagnostics
summary(sdm)

# Predict the species distribution!
map <- predict(sdm, climate, quiet=TRUE)

# How's it look?
par(mfrow=c(1,2))
plot(random.sp$pa.raster, main='True distribution', 
     box=F, axes=F)
plot(map, main='Predicted probability', 
     box=F, axes=F)
```

Neat! We have a good-looking model. Can we put a few summary stats on that??

```{r}
# How good is it?
summary(sdm) 
```

Let's compare what the thresholded model looks like:

```{r}
# How's it look?
par(mfrow=c(1,2))
plot(random.sp$pa.raster, main='True distribution', 
     box=F, axes=F)
plot(map>0.42, main='Prediced distribution', 
     box=F, axes=F)
```

OK! Not bad at all. Let's unpack this a little further.

## What's going on inside a BART model?

BART is an ensemble of sum-of-tree models. Each tree is individually adjusted in a random process (an MCMC) based on proposed splitting rules or tree restructurings, and then the model adjusts each tree to the residuals of the sum of the rest of the tree. It's got a bit in common with random forests and boosted regression trees in that regard (see the paper for a longer explanation). 

The posterior distribution in BART is generated by the ensemble of those predictions. There's a burn-in period that's normally dropped, and then you might have -- for example -- 1000 instances of a sum-of-200-trees model, each considered a valid posterior draw. So far, we've only worked with the posterior mean but we can add a bit to that:

```{r}

# Predict the species distribution!
map <- predict(sdm, climate, quantiles=c(0.025, 0.975), quiet=TRUE)

# How's it look?
par(mfrow=c(2,2))
par(mar=c(2,1,2,5))
plot(map[[1]], 'Posterior mean', 
     box=F, axes=F)
plot(map[[2]], 'Lower 95% CI bound', 
     box=F, axes=F)
plot(map[[3]], 'Upper 95% CI bound', 
     box=F, axes=F)
plot(map[[3]]-map[[2]], 'Credible interval width', 
     box=F, axes=F)
```

We could think about this as a way of getting at uncertainty. That big hole towards the bottom the model DEFINITELY knows isn't suitable! 

How should we think about each of these draws? One cool trick is to visualize the process of generating them a bit. `plot.mcmc` shows the first few iterations of the model, and the running posterior mean, so you can see that process:

```{r}
plot.mcmc(sdm, climate, iter=5, quiet = TRUE)
```

From this point on I'll let you evaluate as you go, so it doesn't clutter up the vignette printout. But a few things, if you find this interesting.

First, models have a burn-in, and you can turn that off: that'll show you a bit more of the early learning, even though the models tend to converge pretty fast (as you can see!).

Second, models with fewer trees initially struggle a bit more and do perform a little less well, which means if you set that number to say 5 or 10 trees, you'll see the initial learning a bit more.

```{r, eval=FALSE}
# Retrain the model
sdm.tiny <- bart(y.train=occ.df[,'Observed'],
            x.train=occ.df[,xnames],
            keeptrees = TRUE,
            ntree=5, # 5 tree models
            nskip=0) # No burnin
plot.mcmc(sdm.tiny, climate, iter=100)
```

Finally, the manual plotting is a bit jittery in R. You can adjust the pause between maps with the "wait=" argument in `plot.mcmc`, but that's still not very satisfying, or easily exported. If you want to export these and make a pretty movie of your trees learning I suggest doing something like 

```{r, eval=FALSE}
library(animation)
saveGIF(plot.mcmc(sdm, climate, iter=50), movie.name = "Timelapse.gif", interval = 0.15, 
  ani.width = 800, ani.height = 400)
```

This can be a cool way to show folks involved in your work how the model learns, without getting too into the specific of the trees! 

# Variable selection 

We could probably do better. One of the easiest ways is to cut some variables we don't think are performing well. In BART, pushing the models towards a smaller number of trees forces the variables to compete a bit, and preferentially upweights the better ones. We can plot that!

```{r}
# A variable importance diagnostic. What's behaving well?
# This takes a while to run normally! Drop the iter if you want a plot faster with more variance.
varimp.diag(occ.df[,xnames], occ.df[,'Observed'], iter=50, quiet=TRUE)
```

OK. This would suggest that some of the variables, like x5 and x7, are less important while x3 and x4 are more important. But that doesn't give us a concrete list of what to drop. Luckily, embarcadero has a stepwise variable set reduction function. Let's run that, and then retrain the model.

```{r}
# Stepwise variable set reduction
step.model <- variable.step(x.data=occ.df[,xnames], 
                            y.data=occ.df[,'Observed'],
                            quiet=TRUE)
step.model

# Retrain the model
sdm <- bart(x.train=occ.df[, step.model], y.train=occ.df[,'Observed'],
            keeptrees = TRUE)

# Predict the species distribution!
map <- predict(sdm, climate, quiet=TRUE)

# How's it look?
par(mfrow=c(1,2))
plot(random.sp$pa.raster, main='True distribution', 
     box=F, axes=F)
plot(map, main='Predicted probability', 
     box=F, axes=F)
```

How do the variable importances look?

```{r}
varimp(sdm, plot=TRUE)
```

# Partial dependence plots with BART

One big "downside" of machine learning that people often complain about is that more complex methods are black boxes. This seems to especially come up for classification and regression tree methods that are too complex for you to visualize the entire tree set (e.g., anything more pwoerful than a basic random forest). This is what partial dependence plots are designed for - you can take a look at the individual response curves of any given predictor and the probability of presence.

When we generated our virtual species, we gave it a Gaussian niche that exists as a hypervolume in a multidimensional predictor space. We can take a look at how `virtualspecies` builds that: 

```{r}
par(mfrow=c(1,1))
plotResponse(random.sp)
```

So, that doesn't tell us a huge amount about the individual response. Let's take a random variable, x4, and see how it looks in a partial dependence plot:

```{r}
p <- partial(sdm, x.vars=c('x4'),
        trace=FALSE,
        ci=FALSE)
```

Partial dependence plots show the evaluation of the sum-of-trees model, for all other sets of observed predictor values, at each value of the predictor-of-interest. In this case, the curve being generated shows the posterior median - each individual sum-of-trees model in the posterior can be evaluated and used to generate a curve. You can turn those on with the `trace` argument:

```{r}
p <- partial(sdm, x.vars=c('x4'),
        trace=TRUE,
        ci=FALSE)
```

Alternately, like with the predict() function, you could just pull some credible intervals: 

```{r}
p <- partial(sdm, x.vars=c('x4'),
        trace=FALSE,
        ci=TRUE)
p
```

This defaults to giving you a 95\% credible interval, which you can adjust using the ciwidth argument. 

We could probably make this prettier, too. `dbarts` defaults to spacing the x values based on the spacing of the data, which I don't personally love (it's fun to nitpick!); we also could make this way smoother. Here's how I normally like to make my partials:


```{r}
p <- partial(sdm, x.vars=c('x4'),
        trace=FALSE,
        ci=TRUE,
        smooth=10,
        equal=TRUE)

p[[1]] + ggtitle('Partial plot') + labs(x='x4')
```

Above, I specify "1" because partial actually returns a *list* of partials. This is a useful feature because you can do sometihng like this:


```{r}
p <- partial(sdm,
        trace=FALSE,
        ci=TRUE,
        smooth=10,
        equal=TRUE,
        panels=TRUE)
p
```

It's worth pointing out this captures WAY more uncertainty (and a bit more of the Gaussian looking shape of all this) than most other classification tree methods do. For example, you can run a quick boosted regression tree model and check out how the partial dependence plot for x4 would look:

```{r}
gbm1 <- gbm.step(data=occ.df, gbm.x = 2:5, gbm.y = 1, 
                 family = "bernoulli",
                 tree.complexity = 5, 
                 learning.rate = 0.01, 
                 bag.fraction = 0.5)
par(mfrow=c(1,1))
gbm.plot(gbm1, variable.no=4, rug=TRUE,
         #main="BRT partial",
         plot.layout=c(1,1))
```

Normally in a BRT implementation you would do, say, 500 submodels and then stack these all together. It's a bit graphically inconvenient compared to the ease of doing trace=TRUE, and it has a very different epistemology in terms of what uncertainty is and where it comes from.

There's a few other cool graphical tools you can use in `dbarts` like a two dimensional partial dependence plot (pd2bart), which aren't worth going into here. But `embarcadero` has one last convenience tool worth mentioning, which is a _spartial_ dependence plot - that is, a spatial projection of the partial dependence plot. This is useful for asking, _where_ is x variable suitable for y species? You can generate one of these with `spartial`:

```{r}
spartial(sdm, climate, x.vars='x4',
         equal=TRUE, smooth=5)
```

This is I think way more interesting to look at on a real landscape, but the functionality is there, and has some of the same tricks for spacing the x values as the partial() code does. 

That's most of the important things! Ternary partials (tertials!) are incoming shortly, the code exists but isn't packaged yet. If you have more questions or want to see how this works in greater depth, I recommend the advanced vignette, in the supplement of the `embarcadero` paper (the data and code is available at github.com/cjcarlson/pier39). Have fun aboard the BART!

