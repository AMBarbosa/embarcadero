
---
title: "Using BART with a virtual species!"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{virtualbart}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
suppressMessages(library(tidyverse))
```


# Getting Started

So you're interested in using \texttt{embarcadero} to do species distribution modeling with Bayesian additive regression trees! That's great. BARTs are a powerful way to do machine learning and, while not a new method per se, they are very new for SDMs. 

Most of the core functionality of \texttt{embarcadero} is actually a wrapper for \texttt{dbarts}, which runs the actual BART fitting process. This vignete will show you

1. How to run BARTs
1. Variable importance measures
1. Automated variable selection
1. Partial dependence plots
1. Visualizing the posterior distribution

There's also just going to be some general comments on the process of using BARTs, the challenges to working with them, and some things that are hopefully coming next.

If you want to install, do it using devtools for now:

```{r setup, echo=TRUE}
#devtools::install_github('cjcarlson/embarcadero')
library(embarcadero, quietly = T)
library(NLMR, quietly = T)
library(raster, quietly = T)
library(virtualspecies, quietly = T)
set.seed(12345)
```

Doors are closing; please stand clear of the doors.

# Creating the virtual species

First, let's create an imaginary landscape. We do this using the NLMR package:


```{r}
onelandscape <- function(x) {NLMR::nlm_gaussianfield(nrow = 150,
                                                     ncol = 150,
                                                     rescale = FALSE)}
climate <- stack(lapply(c(1:8), onelandscape))
names(climate) <- c('x1','x2','x3','x4','x5','x6','x7','x8')

plot(climate[[1]],main='An imaginary variable')
```

Next, let's make a species using the 'virtualspecies' package. Our imaginary species will only responds to variables 1-4, making variables 5-8 uninformative predictors (hopefully our model will drop them):

```{r}

# Generate the species' climatic niche

random.sp <- generateRandomSp(climate[[1:4]], 
                              # ^ These are the informative predictors
                              approach="pca",
                              relations='gaussian',
                              species.prevalence=0.5,
                              realistic.sp = TRUE,
                              PA.method='threshold')

# Generate some presences, and some absences, with imperfect detection

sp.points <- sampleOccurrences(random.sp,
                               n=250,
                               type = 'presence-absence')

# Extract the associated climate values

occ <- SpatialPoints(sp.points$sample.points[,c('x','y')])
occ.df <- cbind(sp.points$sample.points,
                raster::extract(climate, occ))

# Finally, let's drop the long-lats and the "Real" ground truthed presence-absence values, and just leave behind an "Observed" and the climate data

occ.df <- occ.df[,-c(1:3)]

```

Alright. Now that we have the dataset, let's get to modeling!

We could easily throw all our data in one model, run it on defaults, make a map, and never think about it again. There's no laws against it.

```{r}
# Train the model
sdm <- bart(y.train=occ.df[,'Observed'],
            x.train=occ.df[,c(2:9)],
            keeptrees = TRUE) # It's very important this is set to TRUE

# Predict the species distribution!
map <- predict(sdm, climate)

# How's it look?
par(mfrow=c(1,2))
plot(random.sp$pa.raster, main='True distribution')
plot(map, main='Predicted probability')
```

This is.... alright? We could probably do better. One of the easiest ways is to cut some variables we don't think are performing well. In BART, pushing the models towards a smaller number of trees forces the variables to compete a bit, and preferentially upweights the better ones. We can plot that!

```{r}
# A variable importance diagnostic. What's behaving well?
# This takes a while to run normally! Drop the iter if you want a plot faster with more variance.
varimp.diag(occ.df[,2:9], occ.df[,'Observed'], iter=50)
```

OK. This would suggest that some of the variables, like x5 and x7, are less important while x3 and x4 are more important. But that doesn't give us a concrete list of what to drop. Luckily, embarcadero has a stepwise variable set reduction function. Let's run that, and then retrain the model.

```{r}
# Stepwise variable set reduction
step.model <- variable.step(x.data=occ.df[,2:9], 
                            y.data=occ.df[,'Observed'])
step.model

# Retrain the model
sdm <- bart.step(xdata=occ.df[, step.model], ydata=occ.df[,'Observed'])

# Predict the species distribution!
map <- predict(sdm, climate)

# How's it look?
par(mfrow=c(1,2))
plot(random.sp$pa.raster, main='True distribution')
plot(map, main='Predicted probability')
```

OK. We have a good-looking model. What's inside?

```{r}
# How good is it?
summary(sdm) 

map <- predict(sdm, climate, quantiles=c(0.01))

# Check 
par(mfrow=c(1,2))
plot(map[[1]], main='Predicted')
plot(random.sp$pa.raster, main='Truth')
#plot(values(map) ~ values(random.sp$pa.raster))
```
